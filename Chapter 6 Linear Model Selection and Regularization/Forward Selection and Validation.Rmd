---
title: "Forward and Backward Selection"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r}
library(ISLR)
summary(Hitters)
```

```{r}
Hitters = na.omit(Hitters)
with(Hitters, sum(is.na(Salary)))
```

Forward Stepwise Selection
============================

Here we use the "regsubsets" function but specify the method = "forward" option:
```{r}
library(leaps)
regfit.fwd = regsubsets(Salary~., data = Hitters, nvmax = 19, method = "forward")
summary(regfit.fwd)
coef(regfit.fwd, 7)
```

We can also specify the method = "backward" option.
```{r}
regfit.bwd = regsubsets(Salary~., data = Hitters, nvmax = 19, method = "backward")
summary(regfit.bwd)
coef(regfit.bwd, 7)
```
Model Selection Using a Validation Set
=======================================

Let's make a training and validation set, so that we choose a good subset model. 
We will do it using a slightly different approach from what was done in the book.
```{r}
dim(Hitters)
set.seed(1)
train = sample(seq(263), 180, replace = FALSE)
train
regfit.fwd = regsubsets(Salary~., data = Hitters[train, ], nvmax=19, method = "forward")
```
Now we will make predictions on the observations not used for training. We know there are 19 models, so we set up some vectors to record the errors. We have to do a bit of work here, because there is no predict method for 'regsubsets'.
```{r}
val.errors = rep(NA, 19)
x.test = model.matrix(Salary~., data = Hitters[-train,]) # notice the - index!
for(i in 1:19){
  coefi = coef(regfit.fwd, id=i)
  pred = x.test[,names(coefi)]%*%coefi
  val.errors[i] = mean((Hitters$Salary[-train]-pred)^2)
}
plot(sqrt(val.errors), ylab="Root MSE", ylim=c(280, 420), pch=19, type="b")
points(sqrt(regfit.fwd$rss[-1]/180), col="blue", pch=19, type="b")
legend("topright", legend=c("Training", "Validation"), col=c("blue", "black"), pch=19)
```

As we expect, the training error goes down monotonically as the model gets bigger, but not so for the validation error.

This was a little tedious - not having a predict method for 'regsubsets'. 
So we will write one!
```{r}
predict.regsubsets = function(object, newdata, id,...){
  form = as.formula(object$call[[2]])
  mat = model.matrix(form, newdata)
  coefi = coef(object, id=id)
  mat[, names(coefi)]%*%coefi
}
```

Model Selection by Cross-Validation
====================================

We will do 10-fold cross-validation. It's really easy!
```{r}
set.seed(11)
folds = sample(rep(1:10, length = nrow(Hitters)))
folds
table(folds)
cv.errors = matrix(NA, 10, 19)
for (k in 1:10){
  best.fit = regsubsets(Salary~., 
    data = Hitters[folds != k,], 
    nvmax=19)
  for (i in 1:19){
    pred = predict(best.fit, Hitters[folds == k,], id=i)
    cv.errors[k, i] = mean((Hitters$Salary[folds == k] - pred)^2)
  }
}
rmse.cv = sqrt(apply(cv.errors, 2, mean))
plot(rmse.cv, pch=19, type="b")
```
